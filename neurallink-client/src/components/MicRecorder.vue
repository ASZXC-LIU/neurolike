<script setup lang="ts">
import { ref, onMounted, onUnmounted } from 'vue';
import { neuralLink } from '../core/NeuralSocket';
import { MessageType } from '../protocol/types';
import type { ServerEmotionShift } from '../protocol/types';
import { agentContext, setAgentStatus } from '../core/stateMachine';
import { bus } from '../core/eventBus';
// å¼•å…¥ hark åº“ (éœ€ç¡®ä¿å·²å®‰è£…: npm install hark @types/hark)
// å¦‚æœæ²¡æœ‰ harkï¼Œæˆ‘ä»¬æš‚æ—¶ç”¨æ¨¡æ‹Ÿé€»è¾‘æˆ–åŸç”Ÿ AudioContext å®ç°
// è¿™é‡Œå‡è®¾æˆ‘ä»¬ä½¿ç”¨åŸç”Ÿ AudioContext å®ç°ç®€å•çš„ VAD é˜ˆå€¼æ§åˆ¶

const isRecording = ref(false);
let mediaRecorder: MediaRecorder | null = null;
let audioStream: MediaStream | null = null;
let audioContext: AudioContext | null = null;
let analyser: AnalyserNode | null = null;
let microphone: MediaStreamAudioSourceNode | null = null;

// ğŸŒŸ Task 1.6: åŠ¨æ€ VAD é˜ˆå€¼
// é»˜è®¤ -50dB (æ ‡å‡†æŠ—å™ª)
const currentVadThreshold = ref(-50); 

// ğŸŒŸ æ ¸å¿ƒä¿®å¤ 1ï¼šåˆ›å»ºä¸€ä¸ªå…¨å±€çš„ Promise é˜Ÿåˆ—ï¼Œå¼ºåˆ¶ä¿è¯å¼‚æ­¥åˆ‡ç‰‡æŒ‰ç»å¯¹é¡ºåºå‘é€
let sendQueue = Promise.resolve();

const blobToBase64 = (blob: Blob): Promise<string> => {
  return new Promise((resolve, reject) => {
    const reader = new FileReader();
    reader.onloadend = () => {
      const dataUrl = reader.result as string;
      const base64 = dataUrl.split(',')[1] || ''; 
      resolve(base64);
    };
    reader.onerror = reject;
    reader.readAsDataURL(blob);
  });
};

const initAudio = async () => {
  try {
    audioStream = await navigator.mediaDevices.getUserMedia({
      audio: { echoCancellation: true, noiseSuppression: false, autoGainControl: false }
    });
    
    // åˆå§‹åŒ– AudioContext ç”¨äº VAD åˆ†æ
    audioContext = new AudioContext();
    analyser = audioContext.createAnalyser();
    analyser.fftSize = 256;
    microphone = audioContext.createMediaStreamSource(audioStream);
    microphone.connect(analyser);
    
    console.log('âœ… [MicRecorder] éº¦å…‹é£ç¡¬ä»¶æƒé™è·å–æˆåŠŸ');
  } catch (err) {
    console.error('âŒ [MicRecorder] æ— æ³•è®¿é—®éº¦å…‹é£:', err);
  }
};

// ğŸŒŸ Task 1.6: ç›‘å¬æœåŠ¡ç«¯æƒ…ç»ªå˜åŒ–ï¼ŒåŠ¨æ€è°ƒæ•´ VAD é˜ˆå€¼
const handleEmotionShift = (payload: ServerEmotionShift) => {
  if (payload.vad_sensitivity !== undefined) {
    // æ˜ å°„é€»è¾‘: sensitivity (0.0-1.0) -> threshold (-60dB to -40dB)
    // 1.0 (æœ€çµæ•) -> -60dB
    // 0.0 (æœ€è¿Ÿé’) -> -40dB
    const newThreshold = -40 - (payload.vad_sensitivity * 20);
    currentVadThreshold.value = Math.round(newThreshold);
    console.log(`ğŸšï¸ [MicRecorder] VAD é˜ˆå€¼å·²æ ¹æ®æƒ…ç»ªè°ƒæ•´ä¸º: ${currentVadThreshold.value}dB (çµæ•åº¦: ${payload.vad_sensitivity})`);
  }
};

const startRecording = () => {
  if (!audioStream) return;
  if (agentContext.status === 'speaking') {
    bus.emit(MessageType.CLIENT_INTERRUPT as any, { reason: 'barge_in' });
    // 2. ğŸŒŸ æ ¸å¿ƒè¡¥é½ï¼šé€šè¿‡ WebSocket è·¨ç«¯ç‹™å‡»åç«¯çš„æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡ï¼
    neuralLink.send(MessageType.CLIENT_INTERRUPT, { reason: 'barge_in' });
  }

  isRecording.value = true;
  setAgentStatus('listening');

  mediaRecorder = new MediaRecorder(audioStream, { mimeType: 'audio/webm' });

  // ğŸŒŸ æ ¸å¿ƒä¿®å¤ 2ï¼šå°†å‘é€ä»»åŠ¡æ’å…¥ä¼ é€å¸¦ï¼Œé˜²æ­¢ Chunk 2 æŠ¢è·‘è¶…è¿‡ Chunk 1
  mediaRecorder.ondataavailable = (event) => {
    if (event.data.size > 0) {
      sendQueue = sendQueue.then(async () => {
        const b64 = await blobToBase64(event.data);
        neuralLink.send(MessageType.CLIENT_AUDIO_CHUNK, {
          audio_b64: b64,
          is_last: false 
        });
      });
    }
  };

  // ğŸŒŸ æ ¸å¿ƒä¿®å¤ 3ï¼šç»“æŸæ ‡å¿—ä¹Ÿå¿…é¡»åœ¨ä¼ é€å¸¦æœ«å°¾æ’é˜Ÿ
  mediaRecorder.onstop = () => {
    sendQueue = sendQueue.then(() => {
      neuralLink.send(MessageType.CLIENT_AUDIO_CHUNK, {
        audio_b64: "",
        is_last: true 
      });
    });
  };

  mediaRecorder.start();
};

const stopRecording = () => {
  if (!mediaRecorder || mediaRecorder.state === 'inactive') return;
  isRecording.value = false;
  setAgentStatus('processing');
  mediaRecorder.stop();
};

onMounted(async () => {
  await initAudio();
  bus.on('system:audio_queue_empty', () => {
    if (agentContext.status === 'speaking') {
      setAgentStatus('idle');
    }
  });
  // æ³¨å†Œæƒ…ç»ªç›‘å¬
  bus.on(MessageType.SERVER_EMOTION_SHIFT, handleEmotionShift);
});

onUnmounted(() => {
  if (audioStream) audioStream.getTracks().forEach(track => track.stop());
  if (audioContext) audioContext.close();
  bus.off(MessageType.SERVER_EMOTION_SHIFT, handleEmotionShift);
});
</script>

<template>
  <div class="mic-panel">
    <div class="status-badge" :class="agentContext.status">
      å½“å‰çŠ¶æ€: {{ agentContext.status.toUpperCase() }}
    </div>
    <div class="vad-indicator">
      å½“å‰å¬è§‰çµæ•åº¦: {{ currentVadThreshold }}dB
    </div>
    <button class="record-btn" :class="{ recording: isRecording }"
      @mousedown="startRecording" @mouseup="stopRecording" @mouseleave="stopRecording">
      {{ isRecording ? 'ğŸ™ï¸ æ­£åœ¨å€¾å¬ (æ¾å¼€å‘é€)...' : 'â¸ï¸ æŒ‰ä½è¯´è¯' }}
    </button>
  </div>
</template>

<style scoped>
/* æ ·å¼ä¿æŒä¸å˜ */
.mic-panel { display: flex; flex-direction: column; align-items: center; gap: 15px; padding: 20px; background: #161b22; border: 1px solid #30363d; border-radius: 8px; }
.status-badge { padding: 5px 15px; border-radius: 20px; font-weight: bold; font-size: 14px; }
.vad-indicator { font-size: 12px; color: #8b949e; }
.idle { background: #484f58; color: white; }
.listening { background: #238636; color: white; }
.processing { background: #d29922; color: black; }
.speaking { background: #8957e5; color: white; }
.record-btn { padding: 15px 30px; font-size: 18px; border-radius: 8px; border: none; background-color: #21262d; color: #c9d1d9; cursor: pointer; transition: all 0.2s; user-select: none; }
.record-btn:active, .record-btn.recording { background-color: #da3633; transform: scale(0.95); box-shadow: 0 0 15px rgba(218, 54, 51, 0.5); }
</style>
