import os
import sys
import io
import numpy as np
import soundfile as sf
import torch
from fastapi import FastAPI, Query
from fastapi.responses import Response
from contextlib import asynccontextmanager
import traceback


# ==========================================
# 1. ç¯å¢ƒè‡ªä¸¾å™¨ (Zerolan é£æ ¼çš„é¢„æ£€ä¸è‡ªæ„ˆ)
# ==========================================
class EnvironmentBootstrapper:
    @staticmethod
    def setup():
        print("ğŸ›¡ï¸ [Zerolan é£æ ¼] æ­£åœ¨è¿›è¡Œç¯å¢ƒè‡ªä¸¾ä¸é¢„æ£€...")

        # 1. è·¯å¾„å®šä¹‰
        SERVER_ROOT = r"E:\TOOL\AI\neurolike\NeuralLink_Server"
        GPT_SOVITS_ROOT = os.path.join(SERVER_ROOT, "GPT-SoVITS")
        GPT_LOGIC_DIR = os.path.join(GPT_SOVITS_ROOT, "GPT_SoVITS")

        # 2. ç¯å¢ƒå˜é‡æ¥ç®¡ (å¼ºåˆ¶è®¾ç½®æ¨¡å‹ç¼“å­˜è·¯å¾„ï¼Œé˜²æ­¢åº•å±‚åº“ä¹±è·‘)
        os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

        # 3. åŠ¨æ€ç³»ç»Ÿè·¯å¾„æ³¨å…¥
        if GPT_SOVITS_ROOT not in sys.path:
            sys.path.insert(0, GPT_SOVITS_ROOT)
        if GPT_LOGIC_DIR not in sys.path:
            sys.path.insert(0, GPT_LOGIC_DIR)

        os.chdir(GPT_SOVITS_ROOT)

        # 4. ğŸ”¥ æ ¸å¿ƒè‡ªæ„ˆé€»è¾‘ï¼šæå‰åˆ›å»ºæ‰€æœ‰å®¹æ˜“æŠ¥é”™çš„ç¼“å­˜æ–‡ä»¶å¤¹
        pretrained_dir = os.path.join(GPT_LOGIC_DIR, "pretrained_models")
        # é’ˆå¯¹ fast_langdetect çš„è‡´å‘½æŠ¥é”™ï¼Œæå‰å»ºå¥½å®ƒçš„ç‹—çª
        fast_langdetect_dir = os.path.join(pretrained_dir, "fast_langdetect")

        folders_to_ensure = [pretrained_dir, fast_langdetect_dir]
        for folder in folders_to_ensure:
            if not os.path.exists(folder):
                os.makedirs(folder, exist_ok=True)
                print(f"ğŸ”§ [è‡ªæ„ˆ] è‡ªåŠ¨åˆ›å»ºç¼ºå¤±çš„åº•å±‚ä¾èµ–ç›®å½•: {folder}")

        print("âœ… ç¯å¢ƒé¢„æ£€é€šè¿‡ï¼Œåº•å±‚è£…ç”²å·²å°±ç»ªã€‚")
        return GPT_LOGIC_DIR, pretrained_dir


# è§¦å‘è‡ªä¸¾
GPT_LOGIC_DIR, PRETRAINED_DIR = EnvironmentBootstrapper.setup()

# ==========================================
# 2. å®‰å…¨å¯¼å…¥æ¨¡å‹å¼•æ“
# ==========================================
try:
    from TTS_infer_pack.TTS import TTS, TTS_Config

    print("ğŸ“¦ æˆåŠŸæŒ‚è½½ GPT-SoVITS çº¯å‡€å¼•æ“")
except ImportError as e:
    print(f"âŒ å¼•æ“å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# ==========================================
# 3. æ ¸å¿ƒé…ç½®ä¸å…¨å±€å•ä¾‹
# ==========================================
GPT_MODEL = os.path.join(PRETRAINED_DIR, "s1v3.ckpt")
SOVITS_MODEL = os.path.join(PRETRAINED_DIR, "s2Gv3.pth")
REF_WAV = os.path.join(PRETRAINED_DIR, "ref_audio.wav")
REF_TEXT = "ä½ å¥½ï¼Œå°æ™ºï¼Œç°åœ¨æ˜¯2026å¹´ã€‚"

tts_pipeline = None


# ==========================================
# 4. ç”Ÿå‘½å‘¨æœŸç®¡ç†
# ==========================================
@asynccontextmanager
async def lifespan(app: FastAPI):
    global tts_pipeline
    print("ğŸ™ï¸ æ­£åœ¨åˆå§‹åŒ–ç¡¬ä»¶ç¥ç»å…ƒ...")
    try:
        config_path = os.path.join(GPT_LOGIC_DIR, "configs", "tts_infer.yaml")
        tts_config = TTS_Config(config_path)

        # è¦†å†™é…ç½®è·¯å¾„
        tts_config.t2s_weights_path = GPT_MODEL
        tts_config.vits_weights_path = SOVITS_MODEL

        # æ™ºèƒ½è®¾å¤‡åˆ†é…
        if torch.cuda.is_available():
            tts_config.device = "cuda"
            tts_config.is_half = True
            print("ğŸš€ [GPU æ¨¡å¼] CUDA å¯ç”¨ï¼Œå‡†å¤‡å…‰é€Ÿæ¨ç†ï¼")
        else:
            tts_config.device = "cpu"
            tts_config.is_half = False
            print("ğŸ¢ [CPU æ¨¡å¼] æœªæ£€æµ‹åˆ° CUDAï¼Œé™çº§ä¸ºæ…¢é€Ÿæ¨ç†ã€‚")

        # å®ä¾‹åŒ–
        print("â³ æ­£åœ¨åŠ è½½ V3 æƒé‡...")
        tts_pipeline = TTS(tts_config)
        print("âœ… è¯­éŸ³ç¥ç»å…ƒå½»åº•å°±ç»ªï¼Œéšæ—¶å¯åˆæˆéŸ³é¢‘ï¼")

    except Exception as e:
        traceback.print_exc()
        print(f"âŒ å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
    yield


app = FastAPI(lifespan=lifespan)


# ==========================================
# 5. API è·¯ç”±å±‚ (Facade æ¨¡å¼)
# ==========================================
@app.get("/tts")
async def generate_tts(text: str = Query(..., description="è¦åˆæˆçš„æ–‡å­—")):
    print(f"ğŸ”® å¼•æ“æ¥æ”¶åˆæˆä»»åŠ¡: {text}")
    try:
        req = {
            "text": text,
            "text_lang": "zh",
            "ref_audio_path": REF_WAV,
            "prompt_text": REF_TEXT,
            "prompt_lang": "zh",
            "top_k": 5,
            "top_p": 1.0,
            "temperature": 1.0,
            "text_split_method": "cut5",
            "batch_size": 1,
            "speed_factor": 1.0,
            "split_bucket": True,
            "return_fragment": False
        }

        audio_data_list = []
        final_sr = 32000

        for sr, chunk in tts_pipeline.run(req):
            final_sr = sr
            audio_data_list.append(chunk)

        if not audio_data_list:
            raise ValueError("æ¨¡å‹æœªç”Ÿæˆä»»ä½•å£°éŸ³ä¿¡å·")

        full_audio = np.concatenate(audio_data_list, axis=0)
        buffer = io.BytesIO()
        sf.write(buffer, full_audio, final_sr, format='WAV')

        print("âš¡ è¯­éŸ³åˆæˆæˆåŠŸï¼Œå·²ä¸‹å‘ï¼")
        return Response(content=buffer.getvalue(), media_type="audio/wav")

    except Exception as e:
        traceback.print_exc()
        print(f"âŒ æ¨ç†å´©æºƒ: {e}")
        return {"error": str(e)}


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="127.0.0.1", port=9880)