import json
import logging
import base64
import re
import time
from enum import Enum
from typing import Dict, Any
import httpx
import io
import uuid
from pydub import AudioSegment
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from pydantic import BaseModel
from funasr import AutoModel
import google.generativeai as genai
import asyncio

# ==========================================
# 0. åˆå§‹åŒ–ä¸æ¨¡å‹åŠ è½½
# ==========================================
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger("NeuralLink_Brain")

logger.info("ğŸ§  æ­£åœ¨åŠ è½½ SenseVoiceSmall...")
asr_model = AutoModel(model="iic/SenseVoiceSmall", trust_remote_code=True, device="cuda:0")


def process_and_recognize(audio_bytes: bytes) -> list:
    """è´Ÿè´£å°†éæ ‡å‡† WebM/PCM å­—èŠ‚æµæ´—æˆ 16kHz WAVï¼Œå¹¶ç›´æ¥å–‚ç»™å¤§æ¨¡å‹"""
    audio_io = io.BytesIO(audio_bytes)
    audio_segment = AudioSegment.from_file(audio_io)

    # å¼ºåˆ¶é‡é‡‡æ ·ä¸º ASR é»„é‡‘æ ‡å‡†
    audio_segment = audio_segment.set_frame_rate(16000).set_channels(1).set_sample_width(2)

    wav_io = io.BytesIO()
    audio_segment.export(wav_io, format="wav")
    wav_bytes = wav_io.getvalue()

    # ç›´æ¥è°ƒç”¨å…¨å±€çš„ asr_model
    return asr_model.generate(input=wav_bytes, cache={}, language="zh", use_itn=True)
genai.configure(api_key="AIzaSyA8n3UNUSg-4Ln9WWY_jtvUxmxRH4FMwjQ")


# ==========================================
# 1. åè®®æšä¸¾ (ä¿®å¤äº†ä¸å‰ç«¯ä¸åŒ¹é…çš„æšä¸¾)
# ==========================================
class MessageType(str, Enum):
    CLIENT_AUTH_HANDSHAKE = "client.auth_handshake"
    CLIENT_WAKE_UP = "client.wake_up"
    CLIENT_AUDIO_CHUNK = "client.audio_chunk"
    CLIENT_TEXT_REQUEST = "client.text_request"  # ğŸŒŸ æ–°å¢ï¼šæºå¸¦è®°å¿†çš„æ–‡æœ¬è¯·æ±‚
    CLIENT_INTERRUPT = "client.interrupt"  # ğŸŒŸ æ–°å¢ï¼šç¡®ä¿æ‰“æ–­æŒ‡ä»¤åœ¨æšä¸¾ä¸­
    SERVER_ASR_RESULT = "server.asr_result"
    SERVER_THOUGHT_STREAM = "server.thought_stream"
    SERVER_TTS_AUDIO = "server.tts_audio"  # ğŸŒŸ ä¿®å¤ç‚¹ï¼šä¸å‰ç«¯ä¿æŒç»å¯¹ä¸€è‡´
    SERVER_EMOTION_SHIFT = "server.emotion_shift" # ğŸŒŸ Task 1.6 æ–°å¢ï¼šæƒ…ç»ªçŠ¶æ€ä¸‹å‘
    SERVER_ERROR = "server.error"


class WsMessage(BaseModel):
    msg_id: str
    type: MessageType
    timestamp: int
    payload: Dict[str, Any]


class ClientAudioChunk(BaseModel):
    audio_b64: str
    is_last: bool


# ==========================================
# 2. æ ¸å¿ƒé€»è¾‘å¤„ç†å¼•æ“
# ==========================================
class NeuralLinkEngine:
    # ğŸŒŸ æ–°å¢ï¼šç±»çº§åˆ«çš„å…¨å±€ç¼“å­˜ï¼Œå­˜æ”¾å¡«å……éŸ³çš„ Base64ï¼Œå…¨å±€å¤ç”¨
    _filler_audio_b64: str = None

    def __init__(self, websocket: WebSocket):
        self.ws = websocket
        self.is_authenticated = False
        self.audio_buffer = []  # éŸ³é¢‘æ‹¼æ¥ç¼“å­˜åŒº

        self.current_task_id = None
    async def send_message(self, msg_id: str, msg_type: MessageType, payload: dict):
        msg = {
            "msg_id": msg_id,
            "type": msg_type,
            "timestamp": int(time.time() * 1000),
            "payload": payload
        }
        await self.ws.send_text(json.dumps(msg))

    async def run_llm_inference(self, msg_id: str, user_text: str, chat_history: list, task_id: str):
        logger.info(f"âœ¨ æ­£åœ¨é€šè¿‡ Gemini æ€è€ƒ: {user_text}")

        # ğŸŒŸ åŠ¨æ€æ„å»ºä¸Šä¸‹æ–‡è®°å¿†å­—ç¬¦ä¸²
        history_prompt = ""
        if chat_history:
            history_prompt = "ã€å†å²å¯¹è¯è®°å½•ã€‘\n"
            for msg in chat_history:
                role = "ä¸»äºº" if msg.get("role") == "user" else "å°æ™º"
                history_prompt += f"{role}è¯´ï¼š{msg.get('content')}\n"
            history_prompt += "ã€å†å²ç»“æŸã€‘\n\n"

        prompt = (
            f"ä½ æ˜¯ä¸€ä¸ªåä¸ºå°æ™ºçš„ AI åŠ©æ‰‹ã€‚è¯·ä¸è¦åœ¨å›å¤ä¸­ä½¿ç”¨ Emoji è¡¨æƒ…ï¼Œç¡®ä¿æ–‡æœ¬çº¯å‡€ä»¥ä¾¿è¯­éŸ³åˆæˆã€‚\n"
            f"æ ¼å¼å¦‚ä¸‹ï¼š{{\"thought\": \"å†…å¿ƒç‹¬ç™½\", \"speak\": \"ä½ è¦è¯´çš„è¯\"}}ã€‚\n"
            f"æ³¨æ„ï¼šä½ çš„ thought æ€è€ƒè¿‡ç¨‹ä¸¥æ ¼æ§åˆ¶åœ¨ 30 å­—ä»¥å†…ï¼Œç²¾ç®€æ‰¼è¦ï¼\n"
            f"ğŸŒŸ æƒ…æ„Ÿè¡¨è¾¾ï¼šä½ å¯ä»¥ä½¿ç”¨ [speed=1.2] (åŠ å¿«) æˆ– [speed=0.8] (å‡æ…¢) æ¥æ§åˆ¶è¯­é€Ÿï¼Œä¾‹å¦‚ï¼š'[speed=1.5]å¤ªæ£’äº†ï¼'\n"
            f"ğŸŒŸ æƒ…ç»ªçŠ¶æ€ï¼šè¯·åœ¨ thought ä¸­æ˜ç¡®ä½ çš„æƒ…ç»ªçŠ¶æ€ (Mood)ï¼Œä¾‹å¦‚ï¼š'Mood: Happy', 'Mood: Angry', 'Mood: Sad', 'Mood: Neutral'ã€‚\n"
            f"ç”¨æˆ·è¯´ï¼š{user_text}"
        )
        try:
            model = genai.GenerativeModel("gemini-3-pro-preview")
            # ğŸŒŸ æ ¸å¿ƒçªç ´ 1ï¼šçœŸå®å¼€å¯æµå¼æ¥æ”¶
            response_stream = await model.generate_content_async(prompt, stream=True)

            # æµå¼çŠ¶æ€æ¸¸æ ‡
            full_text = ""
            emitted_thought_len = 0
            emitted_speak_len = 0
            sentence_buffer = ""
            sentence_id = 0

            #  æ ¸å¿ƒçªç ´ 2ï¼šTTS å¼‚æ­¥ä¼ é€å¸¦ (Queue)
            tts_queue = asyncio.Queue()
            #  Task 1.4 æ–°å¢ï¼šæ˜¯å¦å·²ç»ä¸‹å‘äº†ç¬¬ä¸€å¥æ­£å¼è¯­éŸ³çš„æ ‡å¿—
            task_state = {"first_audio_sent": False}

            #  Task 1.4 æ–°å¢ï¼šå»¶è¿Ÿæ©ç›–çœ‹é—¨ç‹—åç¨‹
            async def latency_mask_worker():
                # å€’è®¡æ—¶ 600ms (äººç±»å¿å—æ²‰é»˜çš„é˜ˆå€¼)
                await asyncio.sleep(0.6)

                # å¦‚æœ 600ms åï¼Œç¬¬ä¸€å¥æ­£å¼å›å¤è¿˜æ²¡ç”Ÿæˆï¼Œä¸”ç”¨æˆ·æ²¡æœ‰æ‰“æ–­
                if not task_state["first_audio_sent"] and self.current_task_id == task_id:
                    logger.info("â³ æ€è€ƒæ—¶é—´è¶…è¿‡ 600msï¼Œè§¦å‘å»¶è¿Ÿæ©ç›–æœºåˆ¶...")

                    # å¦‚æœå†…å­˜é‡Œè¿˜æ²¡æœ‰ç¼“å­˜å¡«å……éŸ³ï¼Œå» 3060 èŠ‚ç‚¹é™é»˜ç”Ÿæˆä¸€æ¬¡
                    if not NeuralLinkEngine._filler_audio_b64:
                        try:
                            async with httpx.AsyncClient() as client:
                                res = await client.get("http://127.0.0.1:9880/tts", params={"text": "å—¯â€¦â€¦"},
                                                       timeout=3.0)
                                if res.status_code == 200:
                                    NeuralLinkEngine._filler_audio_b64 = base64.b64encode(res.content).decode('utf-8')
                        except Exception as e:
                            logger.warning(f"å¡«å……éŸ³è·å–å¤±è´¥ï¼Œè·³è¿‡æ©ç›–: {e}")
                            return

                    # å†æ¬¡æ ¡éªŒçŠ¶æ€ï¼Œé˜²æ­¢åœ¨è¯·æ±‚ TTS æœŸé—´å‘ç”Ÿæ”¹å˜
                    if not task_state["first_audio_sent"] and self.current_task_id == task_id and NeuralLinkEngine._filler_audio_b64:
                        logger.info("ğŸ‘„ ä¸‹å‘å¡«å……éŸ³: å—¯â€¦â€¦")
                        await self.send_message(msg_id, MessageType.SERVER_TTS_AUDIO, {
                            "audio_b64": NeuralLinkEngine._filler_audio_b64,
                            "sync_text": "å—¯â€¦â€¦",  # å‰ç«¯å¯ä»¥é™é»˜æ˜¾ç¤ºï¼Œæˆ–ä½œä¸ºç‰¹æ•ˆ
                            "sentence_id": 0,  # 0 ä»£è¡¨è¿™æ˜¯ä¸€ä¸ªè¾…åŠ©éŸ³
                            "is_reply_end": False
                        })

            # å¯åŠ¨çœ‹é—¨ç‹—ä»»åŠ¡
            mask_task = asyncio.create_task(latency_mask_worker())

            # --- å®šä¹‰åå°æ¶ˆè´¹è€…ï¼šå¼‚æ­¥æ‹‰å–å¥å­ï¼Œè¯·æ±‚ TTSï¼Œæ¨ç»™å‰ç«¯ ---
            async def tts_worker():
                while True:
                    if self.current_task_id != task_id:
                        break  # ä»»åŠ¡è¢«æ‰“æ–­ï¼Œæ¶ˆè´¹è€…ç›´æ¥ä¸‹ç­

                    item = await tts_queue.get()
                    if item is None:  # æ”¶åˆ°æ¯’è¯ï¼ˆç»“æŸä¿¡å·ï¼‰ï¼Œå®‰å…¨é€€å‡º
                        tts_queue.task_done()
                        break

                    s_id, text_chunk = item
                    logger.info(f"ğŸ™ï¸ æ­£åœ¨å‘ 3060 èŠ‚ç‚¹è¯·æ±‚è¯­éŸ³åˆæˆ [{s_id}]: {text_chunk}")
                    
                    # ğŸŒŸ Task 1.5: SSML æ ‡ç­¾è§£æå™¨
                    speed_factor = 1.0
                    # æå– [speed=1.2]
                    speed_match = re.search(r'\[speed=([\d\.]+)\]', text_chunk)
                    if speed_match:
                        try:
                            speed_factor = float(speed_match.group(1))
                            # å‰”é™¤æ ‡ç­¾ï¼Œåªä¿ç•™çº¯æ–‡æœ¬
                            text_chunk = re.sub(r'\[speed=[\d\.]+\]', '', text_chunk)
                            logger.info(f"ğŸš€ åŠ¨æ€è¯­é€Ÿè°ƒæ•´: {speed_factor}x")
                        except ValueError:
                            pass

                    try:
                        async with httpx.AsyncClient() as client:
                            tts_url = "http://127.0.0.1:9880/tts"
                            tts_res = await client.get(tts_url, params={
                                "text": text_chunk,
                                "speed_factor": speed_factor
                            }, timeout=15.0)

                        if self.current_task_id != task_id:
                            tts_queue.task_done()
                            break  # TTS åˆæˆå›æ¥åï¼Œå†æ¬¡æ£€æŸ¥æ˜¯å¦è¢«æ‰“æ–­

                        if tts_res.status_code == 200:
                            #  Task 1.4 æ–°å¢ï¼šçœŸæ­£çš„æ­£æ–‡è¯­éŸ³å›æ¥äº†ï¼Œç«‹åˆ»å…³é—¨æ‰“ç‹—ï¼
                            if not task_state["first_audio_sent"]:
                                task_state["first_audio_sent"] = True
                                mask_task.cancel()  # å–æ¶ˆçœ‹é—¨ç‹—å€’è®¡æ—¶ï¼ˆå¦‚æœè¿˜æ²¡è§¦å‘çš„è¯ï¼‰

                            audio_b64 = base64.b64encode(tts_res.content).decode('utf-8')
                            await self.send_message(msg_id, MessageType.SERVER_TTS_AUDIO, {
                                "audio_b64": audio_b64,
                                "sync_text": text_chunk,
                                "sentence_id": s_id,
                                "is_reply_end": False
                            })
                    except Exception as e:
                        logger.error(f"âŒ TTS æœåŠ¡æ•…éšœ: {e}")
                    finally:
                        tts_queue.task_done()

            # å¯åŠ¨ TTS æ¶ˆè´¹è€…å¹¶å‘åç¨‹
            tts_task = asyncio.create_task(tts_worker())

            #  æ ¸å¿ƒçªç ´ 3ï¼šç”Ÿäº§è€… (æ­£åˆ™æš´åŠ›æ’•å¼€æœªé—­åˆçš„ JSON)
            async for chunk in response_stream:
                if self.current_task_id != task_id:
                    logger.warning("ğŸ›‘ ä»»åŠ¡å·²ä½œåºŸï¼Œææ–­å¤§è„‘æ€è€ƒæµï¼")
                    break

                try:
                    chunk_text = chunk.text
                except Exception:
                    continue  # è§„é¿ Gemini å®‰å…¨æ‹¦æˆªå¯¼è‡´çš„ chunk æ— æ–‡æœ¬æŠ¥é”™

                if chunk_text:
                    full_text += chunk_text

                    # åŒ¹é… "thought": " å’Œ "speak": " åé¢è·Ÿç€çš„ä»»ä½•éå¼•å·/è½¬ä¹‰å†…å®¹
                    thought_match = re.search(r'"thought"\s*:\s*"((?:[^"\\]|\\.)*)', full_text)
                    speak_match = re.search(r'"speak"\s*:\s*"((?:[^"\\]|\\.)*)', full_text)

                    # --- 1. å¤„ç† Thought ç¢ç‰‡æµ ---
                    if thought_match:
                        current_thought = thought_match.group(1).replace('\\n', '\n')
                        new_thought = current_thought[emitted_thought_len:]
                        if new_thought:
                            await self.send_message(msg_id, MessageType.SERVER_THOUGHT_STREAM, {
                                "chunk": new_thought,
                                "is_end": False
                            })
                            emitted_thought_len = len(current_thought)
                            
                            # ğŸŒŸ Task 1.6: å®æ—¶æƒ…ç»ªæ„ŸçŸ¥ä¸ VAD é˜ˆå€¼ä¸‹å‘
                            # ç®€å•æ­£åˆ™åŒ¹é… Mood: Xxx
                            mood_match = re.search(r'Mood:\s*([A-Za-z]+)', new_thought, re.IGNORECASE)
                            if mood_match:
                                mood = mood_match.group(1).upper()
                                vad_sensitivity = 0.5 # Default Neutral
                                
                                if mood in ["ANGRY", "FOCUS", "EXCITED", "HAPPY"]:
                                    vad_sensitivity = 0.9 # é«˜å”¤é†’ -> é«˜çµæ•åº¦ (-58dB)
                                elif mood in ["SAD", "SLEEPY", "TIRED"]:
                                    vad_sensitivity = 0.2 # ä½å”¤é†’ -> ä½çµæ•åº¦ (-44dB)
                                
                                logger.info(f"ğŸ­ æ„ŸçŸ¥åˆ° AI æƒ…ç»ª: {mood} -> VAD Sensitivity: {vad_sensitivity}")
                                await self.send_message(msg_id, MessageType.SERVER_EMOTION_SHIFT, {
                                    "ai_mood_score": 0, # æš‚ç•™
                                    "live2d_expression": mood.lower(),
                                    "live2d_motion": "nod",
                                    "vad_sensitivity": vad_sensitivity
                                })

                    # --- 2. å¤„ç† Speak ç¢ç‰‡æµå¹¶ã€æ ‡ç‚¹æˆªæ–­ã€‘ ---
                    if speak_match:
                        current_speak = speak_match.group(1).replace('\\n', '\n')
                        new_speak = current_speak[emitted_speak_len:]

                        if new_speak:
                            sentence_buffer += new_speak
                            emitted_speak_len = len(current_speak)

                            # æŸ¥æ‰¾æœ€åå‡ºç°çš„æ ‡ç‚¹ç¬¦å·ä½œä¸ºå®‰å…¨åˆ‡åˆ†ç‚¹
                            match = re.search(r'(.*[ã€‚ï¼ï¼Ÿï¼Œ,\.\!\?])(.*)', sentence_buffer, re.DOTALL)
                            if match:
                                ready_to_speak = match.group(1).strip()
                                sentence_buffer = match.group(2)  # æ²¡å¿µå®Œçš„åŠå¥è¯ç•™åœ¨ç¼“å†²åŒº

                                if ready_to_speak:
                                    sentence_id += 1
                                    # å°†åˆ‡å¥½çš„å¥å­æ‰”ä¸Š TTS ä¼ é€å¸¦ï¼Œè®©åç¨‹å»è·‘
                                    await tts_queue.put((sentence_id, ready_to_speak))

            # --- æµå¼æ¥æ”¶å®Œæ¯•ï¼Œå¤§æ”¶å°¾ ---
            if self.current_task_id == task_id:
                # 1. å¹¿æ’­ thought ç»“æŸä¿¡å·
                await self.send_message(msg_id, MessageType.SERVER_THOUGHT_STREAM, {
                    "chunk": "",
                    "is_end": True
                })

                # 2. æ¸…ç©ºç¼“å†²åŒºé‡Œæ²¡æœ‰æ ‡ç‚¹ç¬¦å·ç»“å°¾çš„æœ€åå‡ ä¸ªå­—
                if sentence_buffer.strip():
                    sentence_id += 1
                    await tts_queue.put((sentence_id, sentence_buffer.strip()))

                # 3. å¾€é˜Ÿåˆ—æ‰”ä¸€ä¸ª None ä½œä¸ºæ¯’è¯ï¼Œé€šçŸ¥ TTS æ¶ˆè´¹è€…å®‰å…¨ä¸‹çº¿
                await tts_queue.put(None)

                # 4. æŒ‚èµ·ç­‰å¾…æ‰€æœ‰ TTS ä»»åŠ¡åˆæˆå®Œæ¯•
                await tts_task

                # 5. ã€å¾®è§‚è¡¥é½ã€‘å‘é€å¯¹è¯ç»“æŸçš„ç©ºåŒ…ï¼Œé‡Šæ”¾å‰ç«¯çŠ¶æ€æœº
                if self.current_task_id == task_id:
                    await self.send_message(msg_id, MessageType.SERVER_TTS_AUDIO, {
                        "audio_b64": "",
                        "sync_text": "",
                        "sentence_id": -1,
                        "is_reply_end": True
                    })
                    logger.info("âœ… è¿™ä¸€è½®å¯¹è¯å½»åº•ç»“æŸï¼ŒçŠ¶æ€é‡ç½®ï¼")

        except Exception as e:
            logger.error(f"ğŸ’¥ è®¤çŸ¥é“¾è·¯å´©æºƒ: {str(e)}")
            await self.send_message(msg_id, MessageType.SERVER_ERROR, {"message": "å¤§è„‘ç¥ç»å…ƒè¿æ¥è¶…æ—¶"})
    async def handle_message(self, raw_data: str):
        try:
            # 1. ç¬¬ä¸€æ­¥æ°¸è¿œæ˜¯å…ˆè§£æ JSON ä¿¡å°
            envelope = WsMessage(**json.loads(raw_data))
            msg_type = envelope.type
            payload = envelope.payload
            msg_id = envelope.msg_id
        except Exception as e:
            logger.error(f"è§£æå¤±è´¥ï¼Œéåˆæ³• JSON: {e}")
            return

        # 2. ä¼˜å…ˆå¤„ç†é«˜ä¼˜å…ˆçº§çš„ç´§æ€¥æ‰“æ–­ä¿¡å·
        if msg_type == MessageType.CLIENT_INTERRUPT:
            logger.warning(f"ğŸ›‘ æ”¶åˆ°ç´§æ€¥æ‰“æ–­ä¿¡å·: {payload.get('reason')}")
            # åˆ·æ–°ä»»åŠ¡ IDï¼Œè®©æ‰€æœ‰æ­£åœ¨è¿›è¡Œçš„æ¨ç†å’Œ TTS å˜æˆâ€œåºŸå¼ƒä»»åŠ¡â€

            self.current_task_id = str(uuid.uuid4())
            return

        # 3. å¤„ç†é‰´æƒ
        if not self.is_authenticated and msg_type == MessageType.CLIENT_AUTH_HANDSHAKE:
            if payload.get("access_token") == "neural_link_secret_2026":
                self.is_authenticated = True
                logger.info("âœ… é‰´æƒæˆåŠŸ")
            return

        # 4. å¤„ç†éŸ³é¢‘æµ
        if msg_type == MessageType.CLIENT_AUDIO_CHUNK:
            chunk = ClientAudioChunk(**payload)
            if chunk.audio_b64:
                self.audio_buffer.append(base64.b64decode(chunk.audio_b64))

            if chunk.is_last:
                logger.info("ğŸ¤ å½•éŸ³æ¥æ”¶å®Œæ¯•ï¼Œå¼€å§‹ ASR...")
                full_audio = b"".join(self.audio_buffer)
                self.audio_buffer = [] # ç»å¯¹æ¸…ç©º

                if len(full_audio) == 0:
                    return

                # ç”Ÿæˆæ–°ä»»åŠ¡çš„ ID
                self.current_task_id = str(uuid.uuid4())
                task_id = self.current_task_id

                try:
                    res = await asyncio.to_thread(process_and_recognize, full_audio)

                    # å®ˆå« 0ï¼šå¦‚æœ ASR æ¨ç†æœŸé—´ç”¨æˆ·æŒ‰äº†æ‰“æ–­ï¼Œç›´æ¥ä¸¢å¼ƒè¯†åˆ«ç»“æœ
                    if self.current_task_id != task_id:
                        logger.info("ä»»åŠ¡å·²ä½œåºŸï¼Œä¸¢å¼ƒ ASR ç»“æœ")
                        return

                    clean_text = re.sub(r'<\|.*?\|>', '', res[0]['text']).strip()
                    
                    # ğŸŒŸ Task 1.5: æå– SenseVoiceSmall çš„æƒ…æ„Ÿæ ‡ç­¾
                    emotion_tags = re.findall(r'<\|(.*?)\|>', res[0]['text'])
                    emotion_context = ""
                    if emotion_tags:
                        # è¿‡æ»¤æ‰éæƒ…æ„Ÿæ ‡ç­¾ (å¦‚ zh, itn ç­‰)
                        valid_emotions = [tag for tag in emotion_tags if tag in ["HAPPY", "SAD", "ANGRY", "NEUTRAL", "SIGH", "LAUGH"]]
                        if valid_emotions:
                            emotion_context = f" (ç”¨æˆ·çŠ¶æ€: {', '.join(valid_emotions)})"
                            logger.info(f"ğŸ­ æ„ŸçŸ¥åˆ°å‰¯è¯­è¨€: {valid_emotions}")

                    logger.info(f"ğŸ‘‚ å¬åˆ°äº†: {clean_text}{emotion_context}")

                    await self.send_message(msg_id, MessageType.SERVER_ASR_RESULT, {
                        "text": clean_text + emotion_context, # å°†æƒ…æ„Ÿæ‹¼æ¥åˆ°æ–‡æœ¬åï¼Œè®©å‰ç«¯ä¹Ÿèƒ½çœ‹è§
                        "is_valid_speech": len(clean_text) > 0
                    })


                except Exception as e:
                    logger.error(f"æ„ŸçŸ¥é“¾è·¯æ•…éšœ: {e}")
 # 5. å¤„ç†å‰ç«¯å‘æ¥çš„å¸¦è®°å¿†çš„å¯¹è¯è¯·æ±‚
        if msg_type == MessageType.CLIENT_TEXT_REQUEST:
            self.current_task_id = str(uuid.uuid4())
            task_id = self.current_task_id

            user_text = payload.get("text", "")
            chat_history = payload.get("chat_history", [])

            await self.run_llm_inference(msg_id, user_text, chat_history, task_id)


# ==========================================
# 3. FastAPI è·¯ç”±
# ==========================================
app = FastAPI()


@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    engine = NeuralLinkEngine(websocket)
    try:
        while True:
            await engine.handle_message(await websocket.receive_text())
    except WebSocketDisconnect:
        logger.info("ğŸ”Œ å®¢æˆ·ç«¯å·²æ–­å¼€è¿æ¥")